syntax = "proto3";

package lbann_data;

message LbannPB {
  DataReader data_reader = 1; 
  Model model = 2; 

  PerformanceParams performance_params = 3;
  SystemParams system_params = 4;
  NetworkParams network_params = 5;
  TrainingParams training_params = 6;

  //use cudnn_manager, if has_cudnn = true, and lbann was compiled with cudnn support
  bool has_cudnn = 7;

  //see note in Layer section below. This is the global optimzer, that is, as of
  //this writing, passed to all pertinent layers. However, in theory we could
  //pass different optimizers to different layers; therefore, each Layer
  //that takes an optimizer in its ctor also has an optimizer field.
  string optimizer = 10; //adagrad, rmsprop, adam, sgd 

  string objective_fn = 11; //categorical_cross_entropy, mean_squared_error
  int32 mini_batch_size = 12;

  //string execution_mode = 13; 
  //   training, validation, testing, prediction, invalid
  //   do we need these? for future development
}

//========================================================================
// start of DataReaders
//========================================================================
message DataReader {
  repeated DataReaderMnist mnist = 1;
  repeated DataReaderCifar10 cifar10 = 2;
  repeated DataReaderImagenet imagenet = 3;
  repeated DataReaderNci nci = 4;
  repeated DataReaderNciRegression nci_regression = 5;
}


message DataReaderMnist {
  string role = 1; //train, test
  int32 batch_size = 2;
  bool shuffle = 3;
  string file_dir = 4;
  string image_file = 5;
  string label_file = 6;
  double percent_samples = 7;
}

message DataReaderCifar10 {
}

message DataReaderImagenet {
}

message DataReaderNci {
}

message DataReaderNciRegression {
}
//========================================================================
// end of DataReaders
//========================================================================

message Model {
  string name = 1; //dnn, stacked_autoencoder, layerwise_autoencoder
  string objective_function = 2;
  int32 mini_batch_size = 6;
  //int32 evaluation_frequency = 3;
  int32 num_epochs = 4;
  //string optimizer = 5;
  repeated string metric = 5;

  repeated Layer layer = 10;
  Optimizer optimizer = 11;

  // checknan, debug, dump_activations, etc;
  // for complete list, see: lbann/include/lbann/callbacks
  repeated string callback = 20; 
}

message Optimizer {
  //adagrad, rmsprop, adam, sgd
  string name = 1;
  double learn_rate = 2;
  double momentum = 3;
  double decay = 4;
  bool nesterov = 5;
}

message Layer {
   int32 index = 2; //corresponds to index wrt std::vector<Layer*>
                     //not currently used

   // a Layer should contain exactly one of the following
   InputDistributedMiniBatchParallelIO input_distributed_minibatch_parallel_io = 8;
   InputDistributedMiniBatch input_distributed_minibatch = 9;
   Input input = 10;
   FullyConnected fully_connected = 11;
   Pooling pooling = 12;
   Convolution convolution = 13;
   Softmax softmax = 14;
   Target target = 15;
   TargetParallel target_parallel = 16;
   TargetDistributedMinibatch target_distributed_minibatch = 17;
   TargetDistributedMinibatchParallelIO target_distributed_minibatch_parallel_io = 18;
}

//========================================================================
// start of Layer types
//========================================================================
//
// weight initialization should be one of: 
//    zero, uniform, normal, glorot_normal, he_normal, he_uniform
// see: lbann/include/lbann/lbann_base.hpp
//
//
// activation_type should be one of:
//    sigmoid, tanh, relu, id, leaky_relu, smooth_relu, elu
// see: lbann/include/lbann/layers/lbann_layer_activations.hpp
//
//
// optimizer should be one of: adagrad, rmsprop, adam, sgd 
//see: lbann/include/lbann/lbann_base.hpp
//
//

message Input {
  int32 mini_batch_size = 1;
}

message InputDistributedMiniBatchParallelIO {
  int32 num_parallel_readers = 1;
  int32 mini_batch_size = 2;
  //@TODO: regularizers
}

message InputDistributedMiniBatch {
  //@TODO
}


message FullyConnected {
  int32 num_prev_neurons = 1;
  int32 num_neurons = 2;
  int32 mini_batch_size = 3;
  string activation_type = 4;
  string weight_initialization = 5;
  string optimizer = 10; 
  repeated string regularizer = 11;
}


message Pooling {
  int32 num_dims = 1;
  int32 num_channels = 2;
  repeated int32 input_dims = 3;
  repeated int32 pool_dims = 4;
  repeated int32 pool_pads = 5;
  repeated int32 pool_strides = 6;

  //pool_mode should be one of: max, average, average_no_pad
  //see: lbann/include/lbann/lbann_base.hpp
  string pool_mode = 7;

  string activation_type = 8;

  //?? std::vector<regularizer*> regs,
}

message Convolution {
  int32 num_dims = 1;
  int32 num_input_channels = 2;
  repeated int32 input_dims = 3;
  int32 num_output_channels = 4;
  repeated int32  filter_dims = 5;
  repeated int32  conv_pads = 6;
  repeated int32  conv_strides = 7;
  int32 mini_batch_size = 8;
  string weight_initialization = 9;
  string activation_type = 10;
   
  //?? std::vector<regularizer*> regs,
}

message Softmax {
  int32 num_prev_neurons = 1;
  int32 num_neurons = 2;
  string weight_initialization = 3;
  string activation_type = 4;
}

message Target {
  //TODO
}

message TargetParallel {
  //TODO
}

message TargetDistributedMinibatch {
  //TODO
}

message TargetDistributedMinibatchParallelIO {
  int32 num_parallel_readers = 1;
  int32 mini_batch_size = 2;
  bool shared_data_reader = 3;
  bool for_regression = 4;
}

//========================================================================
// end of Layer types
//========================================================================


//========================================================================
// start of Params 
//========================================================================

message SystemParams {
  string host_name = 1;
  int32 num_nodes = 2;
  int32 num_cores = 3;
  int32 tasks_per_node = 4;
}

message NetworkParams {
  string network_str = 1;
}

message PerformanceParams {
  int32 block_size = 1;
  int32 max_par_io_size = 2;
}

message TrainingParams {
  bool enable_profiling = 1;
  int32 random_seed = 2;
  int32 shuffle_training_data = 3;
  double percentage_training_samples = 4;
  double percentage_validation_samples = 5;
  double percentage_testing_samples = 6;
  int32 test_with_train_data = 7;
  int32 epoch_start = 8;
  int32 epoch_count = 9;
  int32 mb_size = 10;
  double learn_rate = 11;
  int32 learn_rate_method = 12; //1 - Adagrad, 2 - RMSprop, 3 - Adam
  double lr_decay_rate = 13;  
  int32 lr_decay_cycles = 14;
  double lr_momentum = 15; 
  string activation_type = 16; //sigmoid, tanh, relu, id, leaky_relu, smooth_relu, elu
  double dropout = 17;
  double lambda = 18;
  string weight_initialization = 19; //zero, uniform, normal, glorot_normal, glorot_uniform, he_normal, he_uniform
  string dataset_root_dir = 20;
  string save_image_dir = 21;
  string parameter_dir = 22;
  bool save_model = 23;
  bool load_model = 24;
  int32 ckpt_epochs = 25;
  int32 ckpt_steps = 26;
  int32 ckpt_secs = 27;
  string train_file = 28;
  string test_file = 29;
  string summary_dir = 30;
  bool dump_weights = 31;
  bool dump_activations = 32;
  bool dump_gradients = 33;
  string dump_dir = 34;
  int32 intermodel_comm_method = 35;
  int32 procs_per_model = 36;
}

//========================================================================
// end of Params 
//========================================================================
