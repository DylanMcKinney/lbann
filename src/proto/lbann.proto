syntax = "proto3";

package lbann_data;

message LbannPB {
  DataReader data_reader = 1; 
  Model model = 2; 
}

//========================================================================
// start of DataReaders
//========================================================================
message DataReader {
  int64 max_par_io_size = 1;
  repeated Reader reader = 2;
}

message Reader {
  string name = 1; //mnist, nci, nci_regression, cnpy, imagenet
  string role = 3; //train, test
  bool shuffle = 4;
  string data_filename = 6;
  string label_filename = 7;
  double train_or_test_percent = 8;
  double validation_percent = 9;
  bool firstN = 10;
  int64 max_sample_count = 11;
  double percent_of_data_to_use = 12;
  ImagePreprocessor image_preprocessor = 13;
}

message ImagePreprocessor {
  bool scale = 12;
  bool subtract_mean = 13;
  bool unit_variance = 14;
  bool z_score = 15;
  bool horizontal_flip = 16;
  bool vertical_flip = 17;
  double rotation = 18;
  double horizontal_shift = 19;
  double vertical_shift = 20;
  double shear_range = 21;
  bool disable_augmentation = 22;
}

//========================================================================
// end of DataReaders
//========================================================================

message Model {
  string name = 1; //dnn, stacked_autoencoder, greedy_layerwise_autoencoder
  string objective_function = 2; //categorical_cross_entropy, mean_squared_error
  repeated string metric = 5; //categorical_accuracy, mean_squared_error

  int64 mini_batch_size = 12;
  int64 num_epochs = 4;
  int64 block_size = 50;
  int64 procs_per_model = 51;
  int64 num_parallel_readers = 52;
  int64 num_gpus = 53;
  int64 evaluation_frequency = 54;

  //use cudnn_manager, if use_cudnn=true AND lbann was compiled with cudnn support
  bool use_cudnn = 8;

  //see note in Layer section below. This is the global optimzer, that is, as of
  //this writing, passed to all pertinent layers. However, in theory we could
  //pass different optimizers to different layers; therefore, each Layer
  //that takes an optimizer in its ctor also has an optimizer field.
  Optimizer optimizer = 11;

  repeated Layer layer = 10;

  // checknan, debug, dump_activations, etc;
  // for complete list, see: lbann/include/lbann/callbacks
  repeated Callback callback = 20; 

  //checkpointing
  string checkpoint_dir = 30;
  int64 checkpoint_epochs = 31;
  int64 checkpoint_steps = 32;
  double checkpoint_secs = 33;
}

message Optimizer {
  //adagrad, rmsprop, adam, sgd
  string name = 1;
  double learn_rate = 2;
  double momentum = 3;
  double decay = 4;
  bool nesterov = 5;
}

message Callback {
   // a Callback should contain exactly one of the following
   CallbackPrint print = 1;
   CallbackTimer timer = 2;
   CallbackSummary summary = 3;
   CallbackDumpWeights dump_weights = 4;
   CallbackDumpActivations dump_activations = 5;
   CallbackDumpGradients dump_gradients = 6;
   CallbackImComm imcomm = 7;
   CallbackSaveImages save_images = 8;
}

message CallbackSaveImages {
  string image_dir = 1;
  string extension = 2;
}

message CallbackPrint {
  int64 interval = 1; //default in lbann_callback_print.hpp is 1
}

message CallbackTimer {
  string dir = 1; //directory for the lbann_summary
}

message CallbackSummary {
  string dir = 1; //directory for the lbann_summary
  int64 interval = 2; //default in lbann_callback_summary.hpp is 1
}

message CallbackDumpWeights {
  string basename = 1;
  int64 interval = 2;
}

message CallbackDumpActivations {
  string basename = 1;
  int64 interval = 2;
}

message CallbackDumpGradients {
  string basename = 1;
  int64 interval = 2;
}

message CallbackImComm {
  //todo
}

//========================================================================
// start of Layer types
//========================================================================
//
// weight initialization should be one of: 
//    zero, uniform, normal, glorot_normal, he_normal, he_uniform
// see: lbann/include/lbann/lbann_base.hpp
//
//
// activation_type should be one of:
//    sigmoid, tanh, relu, id, leaky_relu, smooth_relu, elu
// see: lbann/include/lbann/layers/lbann_layer_activations.hpp
//
//
// optimizer should be one of: adagrad, rmsprop, adam, sgd 
//see: lbann/include/lbann/lbann_base.hpp
//
//

message Layer {
   int64 index = 50; 
   int64 parent = 51;

   // a Layer should contain exactly one of the following;
   // this may or may not be properly checked for in lbann_proto_common.cpp

   // input Layers
   InputDistributedMiniBatchParallelIO input_distributed_minibatch_parallel_io = 2;
   InputPartitionedParallel input_partitioned_parallel = 3;

   // target Layers
   //TargetDistributed target_distributed = 100;
   //TargetDistributedParallel target_distributed_parallel = 101;
   //TargetParititionedParellel target_partitioned_parallel = 101;


   FullyConnected fully_connected = 11;
   Pooling pooling = 12;
   Convolution convolution = 13;
   Softmax softmax = 14;
   Target target = 15;
   TargetParallel target_parallel = 16;
   TargetDistributedMinibatch target_distributed_minibatch = 17;
   TargetDistributedMinibatchParallelIO target_distributed_minibatch_parallel_io = 18;

   // regularization Layers
   BatchNormalization batch_normalization = 19;
   LocalResponseNormalization local_response_normalization = 20;
   Dropout dropout = 21;

   // activation Layers
   ELU elu = 30;
   ID id = 31;
   LeakyRelu leaky_relu = 32;
   Relu relu = 33;
   Sigmoid sigmoid = 34;
   SmoothRelu smooth_relu = 35;
   Softplus soft_plus = 36;
}

///////////////////////
// Activation Layers //
///////////////////////
message ELU {
  double alpha = 1; //default: 1.0; must be >= 0
}

message ID {
}

message LeakyRelu {
  double leak = 1; //default: 0.01
}

message Relu {
}

message Sigmoid {
}

message SmoothRelu {
}

message Softplus {
}

message Tanh {
}

///////////////////////////
// Regularization Layers //
///////////////////////////
message BatchNormalization {
  double decay = 2; //default: 0.9
  double gamma = 3; //default: 1.0
  double beta = 4;  //default: 0.0
}

message LocalResponseNormalization {
  int64 num_dims = 1;
  int64 num_channels = 2;
  string dims = 3;  //e.g, "2 2 2"
  int64 window_width = 4;
  double lrn_alphs = 5;
  double lrn_beta = 6;
  double lrn_k = 7;
}

message Dropout {
  double keep_prob = 2;  //default: 0.5
}


//////////////////
// Input Layers //
//////////////////

message InputDistributedMiniBatchParallelIO {
  string data_layout = 1;
}

message InputPartitionedParallel {
  string data_layout = 1;
  int64 num_parallel_readers = 2;
}


message FullyConnected {
  string data_layout = 1;
  int64 num_neurons = 2;
  string weight_initialization = 5;
  bool has_bias = 6;
}


message Pooling {
  string data_layout = 100;
  int64 num_dims = 1;
  int64 num_channels = 2;
  string input_dims = 3; //should be space-separated list, e.g, "2 2 3"
  string pool_dims = 4; //should be space-separated list, e.g, "2 2 3"
  string pool_pads = 5; //should be space-separated list, e.g, "2 2 3"
  string pool_strides = 6; //should be space-separated list, e.g, "2 2 3"

  //pool_mode should be one of: max, average, average_no_pad
  //see: lbann/include/lbann/lbann_base.hpp
  string pool_mode = 7;
}

message Convolution {
  string data_layout = 100;
  int64 num_dims = 1;
  int64 num_input_channels = 2;
  string input_dims = 3; //should be space-separated list, e.g, "2 2 3"
  int64 num_output_channels = 4;
  string filter_dims = 5; //should be space-separated list, e.g, "2 2 3"
  string conv_pads = 6;  //should be space-separated list, e.g, "2 2 3"
  string conv_strides = 7; //should be space-separated list, e.g, "2 2 3"
  int64 mini_batch_size = 8;
  string weight_initialization = 9;
  string activation_type = 10;
}

message Softmax {
  string data_layout = 100;
  int64 num_neurons = 2;
  string weight_initialization = 3;
}

message Target {
  //TODO
}

message TargetParallel {
  //TODO
}

message TargetDistributedMinibatch {
  //TODO
}

message TargetDistributedMinibatchParallelIO {
  string data_layout = 1;
  bool shared_data_reader = 3;
  bool for_regression = 4;
}

//========================================================================
// end of Layer types
//========================================================================
