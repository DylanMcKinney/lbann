/*
 * This prototext file can not be used as input to model_zoo/lbann_proto;
 * instead, it's provided to illustrate all (or most) of the various items
 * that can appear in a prototext file. I'm pretty sure that comments such
 * as this one, and those that appear below, will cause an exception
 * if they're included in an actual prototext file.
 */

model {
  name: "dnn" //dnn, stacked_autoencoder, or greedy_layerwise_autoencoder
  objective_function: "categorical_cross_entropy" //categorical_cross_entropy or mean_squared_error
  metric: "categorical_accuracy" //categorical_accuracy or mean_squared_error
  mini_batch_size: 192 
  num_epochs: 10
  num_parallel_readers: 0
  procs_per_model: 0
  use_cudnn: false
  num_gpus: -1
  evaluation_frequency: 1 //How often to evaluate model on validation set. A value less than 1 will disable evaluation.

  optimizer {
    name: "adagrad" //adagrad, rmsprop, adam or sgd
    learn_rate: 0.01
    momentum: 0.9
    decay: 0.5
    nesterov: true
  }

  /* 
   * "data_layout" fields should be either data_parallel or model_parallel 
   * 
   * activation_type: sigmoid, tanh, relu, id, leaky_relu, smooth_relu or elu
   * (see: include/lbann/layers/lbann_layer_activations.hpp; sigmoid = 1)
   *
   * weight_initialization: zero, uniform, normal, glorot_normal, he_normal or he_uniform
   * (see: include/lbann/lbann_base.hpp; zero = 0
   *
   * Each layer has a unique identifying index. These need not be sequential.
   * For now layers are instantiated wrt the order in which they
   * appear below; in the future they will be instantiated wrt the parent
   * and child fields, and ordering within the prototext file will be ignored
   * -1 indicates there is no parent or child layer
   */

  layer {
    input_distributed_minibatch_parallel_io {
      data_layout: "data_parallel"
    }
    index: -1
    parent: 0
    child: 1
  }

  layer {
    fully_connected {
      data_layout: "data_parallel"
      num_neurons: 100
      activation_type: "sigmoid"
      weight_initialization: "glorot_uniform"
    }
    index: 1
    parent: 0
    child: 2
  }

  layer {
    convolution {
      num_dims: 2
      num_input_channels: 32
      input_dims: "26 26"
      num_output_channels:  32
      filter_dims: "3 3"
      conv_pads: "0 0"
      conv_strides: "1 1"
      activation_type: "relu"
      weight_initialization: "glorot_uniform"
    }
    index: 1
    parent: 0
    child: 2
  }

  layer {
    pooling {
      num_dims: 2
      num_channels: 32
      input_dims: "24 24"
      pool_dims: "2 2"
      pool_pads: "0 0"
      pool_strides: "2 2"
      pool_mode: "max" //max, average, average_no_pad
    }
    index: 1
    parent: 0
    child: 2
  }

  layer {
    softmax {
      data_layout: "data_parallel"
      num_neurons: 10
      weight_initialization: "glorot_uniform"
    }
    index: 1
    parent: 0
    child: 2
  }

  layer {
    target_distributed_minibatch_parallel_io {
      data_layout: "data_parallel"
      shared_data_reader: true
    }
    index: 5
    parent: 4
    child: -1
  }

  /* 
   * except where noted, regularizer values below are the current lbann defaults
   */ 
  regularizer {
    batch_normalization {
      data_layout: model_parallel
      decay: .0.9  
      gama: 1.0
      beta: 0.0
    }
  }
  regularizer {
    dropout {
      data_layout: model_parallel
      keep_prob: 0.5
    }
  }
  regularizer {
    l2_regularization {
      lambda: 0 //no default in
    }
  }

  /*
   * I'm not listing all the callbacks here ... see src/proto/lbann.proto for 
   * aditional info
   */
  callback {
    save_images {
      image_dir: "images"
      extension: "png"
    }
  }
  callback {
    print {
      interval: 1
    }
  }
  callback {
    timer {
      dir: "none"
    }
  }
  callback {
    summary {
      dir: "none"
      interval: 1
    }
  }
}
