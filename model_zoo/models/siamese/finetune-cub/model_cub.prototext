model {
  name: "sequential_model"
  data_layout: "data_parallel"
  mini_batch_size: 64
  block_size: 256
  num_epochs: 50
  num_parallel_readers: 0
  procs_per_model: 0
  use_cudnn: true
  num_gpus: -1

  ###################################################
  # Objective function
  ###################################################

  objective_function {
    cross_entropy {}
    l2_weight_regularization {
      scale_factor: 0.0005
    }
  }

  ###################################################
  # Metrics
  ###################################################

  metric { categorical_accuracy {} }
  metric {
    top_k_categorical_accuracy {
       top_k: 5
    }
  }

  ###################################################
  # Callbacks
  ###################################################
  callback {
    imcomm {
      intermodel_comm_method: "normal"
      all_optimizers: true
    }
  }
  callback { print {} }
  callback { timer {} }
  callback {
    summary {
      dir: "."
      mat_interval: 25
    }
  }

  ###################################################
  # start of replicate layers
  ###################################################

  layer {
    name: "data"
    data_layout: "data_parallel"
    input {
      io_buffer: "partitioned"
    }
  }

  layer {
    name: "conv1_head0"
    data_layout: "data_parallel"
    convolution {
      num_dims: 2
      num_output_channels: 96
      conv_dims: "11 11"
      conv_pads: "0 0"
      conv_strides: "4 4"
      has_bias: true
      has_vectors: true
    }
  }

  layer {
    name: "relu1"
    data_layout: "data_parallel"
    relu {}
  }

  layer {
    name: "pool1"
    data_layout: "data_parallel"
    pooling {
      num_dims: 2
      pool_dims: "3 3"
      pool_pads: "0 0"
      pool_strides: "2 2"
      pool_mode: "max"
      has_vectors: true
    }
  }

  layer {
    name: "norm1"
    data_layout: "data_parallel"
    local_response_normalization {
      window_width: 5
      lrn_alpha: 0.0001
      lrn_beta: 0.75
      lrn_k: 2
    }
  }

  layer {
    name: "conv2_head0"
    data_layout: "data_parallel"
    convolution {
      num_dims: 2
      num_output_channels: 256
      conv_dims: "5 5"
      conv_pads: "2 2"
      conv_strides: "1 1"
      has_bias: true
      has_vectors: true
    }
  }

  layer {
    name: "relu2"
    data_layout: "data_parallel"
    relu {}
  }

  layer {
    name: "pool2"
    data_layout: "data_parallel"
    pooling {
      num_dims: 2
      pool_dims: "3 3"
      pool_pads: "0 0"
      pool_strides: "2 2"
      pool_mode: "max"
      has_vectors: true
    }
  }

  layer {
    name: "norm2"
    data_layout: "data_parallel"
    local_response_normalization {
      window_width: 5
      lrn_alpha: 0.0001
      lrn_beta: 0.75
      lrn_k: 2
    }
  }

  layer {
    name: "conv3_head0"
    data_layout: "data_parallel"
    convolution {
      num_dims: 2
      num_output_channels:  384
      conv_dims: "3 3"
      conv_pads: "1 1"
      conv_strides: "1 1"
      has_bias: true
      has_vectors: true
    }
  }

  layer {
    name: "relu3"
    data_layout: "data_parallel"
    relu {}
  }

  layer {
    name: "conv4_head0"
    data_layout: "data_parallel"
    convolution {
      num_dims: 2
      num_output_channels:  384
      conv_dims: "3 3"
      conv_pads: "1 1"
      conv_strides: "1 1"
      has_bias: true
      has_vectors: true
    }
  }

  layer {
    name: "relu4"
    data_layout: "data_parallel"
    relu {}
  }

  layer {
    name: "conv5_head0"
    data_layout: "data_parallel"
    convolution {
      num_dims: 2
      num_output_channels:  256
      conv_dims: "3 3"
      conv_pads: "1 1"
      conv_strides: "1 1"
      has_bias: true
      has_vectors: true
    }
  }

  layer {
    name: "relu5"
    data_layout: "data_parallel"
    relu {}
  }

  layer {
    name: "pool5"
    data_layout: "data_parallel"
    pooling {
      num_dims: 2
      pool_dims: "3 3"
      pool_pads: "0 0"
      pool_strides: "2 2"
      pool_mode: "max"
      has_vectors: true
    }
  }

  ######################################
  # Start of redefined layers
  ######################################

  layer {
    name: "conv6"
    data_layout: "data_parallel"
    convolution {
      num_dims: 2
      num_output_channels:  4096
      conv_dims: "3 3"
      conv_pads: "1 1"
      conv_strides: "1 1"
      has_bias: true
      has_vectors: true
    }
  }

  layer {
    name: "relu6"
    data_layout: "data_parallel"
    relu {}
  }

  layer {
    name: "conv6b"
    data_layout: "data_parallel"
    convolution {
      num_dims: 2
      num_output_channels:  1024
      conv_dims: "1 1"
      conv_pads: "1 1"
      conv_strides: "1 1"
      has_bias: true
      has_vectors: true
    }
  }

  layer {
    name: "relu6b"
    data_layout: "data_parallel"
    relu {}
  }

  layer {
    name: "pool6"
    data_layout: "data_parallel"
    pooling {
      num_dims: 2
      pool_dims: "3 3"
      pool_pads: "0 0"
      pool_strides: "2 2"
      pool_mode: "max"
      has_vectors: true
    }
  }

  ######################################
  # End of redefined layers
  ######################################

  layer {
    name: "fc7"
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 4096
      has_bias: true
    }
  }

  layer {
    name: "relu7"
    data_layout: "model_parallel"
    relu {}
  }

  layer {
    name: "drop7"
    data_layout: "model_parallel"
    dropout {
      keep_prob: 0.5
    }
  }

  layer {
    name: "fc8"
    data_layout: "model_parallel"
    fully_connected {
      num_neurons: 200
      has_bias: false
    }
  }

  layer {
    name: "prob"
    data_layout: "model_parallel"
    softmax {}
  }

  layer {
    name: "target"
    data_layout: "data_parallel"
    target {
      io_buffer: "partitioned"
      shared_data_reader: true
    }
  }

}
